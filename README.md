# Udacity's Data Engineering nanodegree
******

Hi all,
I'm pleased to share with you my latest accomplishment : the __Udacity's Data Engineering nanodegree__.
Throughout the programme, I enjoyed leaning the skills and knowledge to design, build, maintain, and troubleshoot data pipelines. I also learned a lot from __AWS cloud computing and storing__ capabilities.   
All the 6 projects are detailled below.  
The __capstone project__ was the most exciting as it allows to build a large __data lake__ that combines __stock markets__ and __financial tweets__. It's a huge project that involves collecting, storing, processing, and analyzing large datasets.  
I also implemented this pipeline for my own interests as I will be using the data lake to perform __sentiment analysis__ on financial tweets and news as an enthusiast __data scientist__.  

***
The Data Engineer's certificate can be found [here](udacity_data_engineer_cerificate.pdf).
******
Nanodegree's projects:

*click on the below links to deep dive into the projects*.

1. [Capstone project](https://github.com/Datapyaddict/udacity-project-data-engineer-capstone-project)  
	> In this project, I'm building a pipeline that could fetch automatically stock markets' data from yahoo finance and financial tweets from Twitter via respective APIs on a scheduled basis, retreat them accordingly to fit in a relational data model and dump the final data back into a data lake as `parquet` files.

2. [Automate a data pipeline with Airflow](https://github.com/Datapyaddict/udacity-project-automate-data-pipeline-with-airflow)  
	> In this project, I'm buiding a data pipeline with `Apache Airflow` to meet the requirements defined by a fictitious company __Sparkify__ and I'm making use of `AWS EMR` and `s3` bucket capabilities.    
	
3. [Set up a data lake with Spark](https://github.com/Datapyaddict/udacity-project-data-set-up-a-data-lake-with-spark).  
	> The purpose of this project was to build a pipeline using `Spark`, `python` and `AWS EMR` and `S3` capabilities.  
	
4. [Set up a Datawarehouse](https://github.com/Datapyaddict/udacity-project-data-warehouse).  
	> The purpose of this project is to build a datawarehouse with `AWS Redshift`.  

5. [Data modelling with Apache Cassandra](https://github.com/Datapyaddict/udacity-project-data-modeling-with-cassandra).  
	> The purpose of this project is to model the data by creating tables in `Apache Cassandra` Nosql tool to run queries in order to answer specific questions.  
	This is done through a jupyter notebook.  
6. [Data modelling with Postgres](https://github.com/Datapyaddict/udacity-project-data-modeling-with-postgres)
	> Sparkify, a fictitious startup, wants its analytics team to analyse the data collected on songs and user activity from its music streaming app.  
	The purpose of this project is implement a pipeline in python in order to extract, transform and load the information from JSON files into a `Apache Postgre` database and implement some analytics.